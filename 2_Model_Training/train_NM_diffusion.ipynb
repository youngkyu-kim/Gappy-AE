{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import lr_scheduler\n",
    "import sparselinear as sl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse as sp\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "import sys,time\n",
    "import os\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(\"Using device:\", device, '\\n')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given parameters\n",
    "nx = 60\n",
    "ny = 60\n",
    "m=nx*ny\n",
    "tf = 1.0\n",
    "dt = 2.0e-3\n",
    "nt = int(tf/dt)\n",
    "xmin = 0; xmax = 1\n",
    "ymin = 0; ymax = 1\n",
    "\n",
    "# set batch_size, number of epochs, paitience for early stop\n",
    "batch_size = 240\n",
    "num_epochs = 10000\n",
    "num_epochs_print = 1000\n",
    "early_stop_patience = 200\n",
    "num_patience = 50\n",
    "l_rate = 0.001\n",
    "\n",
    "# Choose data normalize option (option 1: -1<=X<=1 option 2: 0<=X<=1, option 3: no scaling)\n",
    "option = 3\n",
    "\n",
    "# Choose activation function (sigmoid, swish)\n",
    "activation = 'swish'\n",
    "\n",
    "# parameters\n",
    "training_params = np.arange(80,121,5)\n",
    "print(\"nt={}\".format(nt))\n",
    "print(\"training parameters={}\".format(training_params))\n",
    "\n",
    "# train:test split\n",
    "r_test=0.2\n",
    "\n",
    "# set the number of nodes in each layer\n",
    "a = 2\n",
    "b = int(100)\n",
    "db = int(10)\n",
    "\n",
    "M1 = int(a*m) # encoder hidden layer\n",
    "M2 = b + (m-1)*db # decoder hidden layer\n",
    "\n",
    "# latent space dimension\n",
    "f_list=np.array([3,4,5,6])\n",
    "\n",
    "# generate mesh grid\n",
    "[xv,yv]=np.meshgrid(np.linspace(xmin,xmax,nx),np.linspace(ymin,ymax,ny),indexing='xy')\n",
    "x=xv.flatten()\n",
    "y=yv.flatten()\n",
    "\n",
    "multi_index_i,multi_index_j=np.meshgrid(np.arange(nx),np.arange(ny),indexing='xy')\n",
    "full_multi_index=(multi_index_j.flatten(),multi_index_i.flatten())\n",
    "free_multi_index=(multi_index_j[1:-1,1:-1].flatten(),multi_index_i[1:-1,1:-1].flatten())\n",
    "x0_multi_index=(multi_index_j[1:-1,0].flatten(),multi_index_i[1:-1,0].flatten())\n",
    "x1_multi_index=(multi_index_j[1:-1,-1].flatten(),multi_index_i[1:-1,-1].flatten())\n",
    "y0_multi_index=(multi_index_j[0,1:-1].flatten(),multi_index_i[0,1:-1].flatten())\n",
    "y1_multi_index=(multi_index_j[-1,1:-1].flatten(),multi_index_i[-1,1:-1].flatten())\n",
    "\n",
    "dims=(ny,nx)\n",
    "full_raveled_indicies=np.ravel_multi_index(full_multi_index,dims)\n",
    "free_raveled_indicies=np.ravel_multi_index(free_multi_index,dims)\n",
    "x0_raveled_indicies=np.ravel_multi_index(x0_multi_index,dims)\n",
    "x1_raveled_indicies=np.ravel_multi_index(x1_multi_index,dims)\n",
    "x01_raveled_indicies=np.concatenate((x0_raveled_indicies,x1_raveled_indicies))\n",
    "y0_raveled_indicies=np.ravel_multi_index(y0_multi_index,dims)\n",
    "y1_raveled_indicies=np.ravel_multi_index(y1_multi_index,dims)\n",
    "y01_raveled_indicies=np.concatenate((y0_raveled_indicies,y1_raveled_indicies))\n",
    "fixed_raveled_indicies=np.setdiff1d(full_raveled_indicies,free_raveled_indicies)\n",
    "\n",
    "# measurements\n",
    "msmt_idx=full_raveled_indicies\n",
    "\n",
    "# Gauss-Newton iteration parameters\n",
    "maxitr=4\n",
    "tol=1e-8\n",
    "\n",
    "# random number generator seed\n",
    "seed=np.random.randint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load snapshot\n",
    "snapshot = np.array([])\n",
    "for i in training_params:\n",
    "    ex = np.load('./diffusion_data/ex16_interp_{}.npz'.format(i), allow_pickle = True)\n",
    "    ex = ex.f.arr_0\n",
    "    snapshot = np.append(snapshot, ex)\n",
    "snapshot = snapshot.reshape(len(training_params)*(nt+1),-1).astype('float32')\n",
    "print(snapshot.shape,snapshot.dtype)\n",
    "\n",
    "# number of data points\n",
    "ndata = snapshot.shape[0]\n",
    "\n",
    "# generate mesh grid\n",
    "[xv,yv]=np.meshgrid(np.linspace(xmin,xmax,nx),np.linspace(ymin,ymax,ny),indexing='xy')\n",
    "x=xv.flatten()\n",
    "y=yv.flatten()\n",
    "\n",
    "multi_index_i,multi_index_j=np.meshgrid(np.arange(nx),np.arange(ny),indexing='xy')\n",
    "full_multi_index=(multi_index_j.flatten(),multi_index_i.flatten())\n",
    "dims=(ny,nx)\n",
    "full_raveled_indicies=np.ravel_multi_index(full_multi_index,dims)\n",
    "\n",
    "orig_data_u=np.copy(snapshot)\n",
    "\n",
    "# normalize data\n",
    "if option==1: # option 1: -1<=X<=1\n",
    "    print(\"option {}: -1<=X<=1\".format(option))\n",
    "    u_ref = (np.max(orig_data_u,axis=0)+np.min(orig_data_u,axis=0))/2.0\n",
    "    u_scale = (np.max(orig_data_u,axis=0)-np.min(orig_data_u,axis=0))/2.0\n",
    "    u_scale = np.where(u_scale > np.finfo('float32').resolution, u_scale, 1)\n",
    "    u_scale_reciprocal = np.reciprocal(u_scale)  \n",
    "    data_u = u_scale_reciprocal*(orig_data_u-u_ref)\n",
    "    \n",
    "elif option==2: # option 2: 0<=X<=1\n",
    "    print(\"option {}: 0<=X<=1\".format(option))\n",
    "    u_ref = np.min(orig_data_u,axis=0)    \n",
    "    u_scale = np.max(orig_data_u,axis=0)-np.min(orig_data_u,axis=0)    \n",
    "    u_scale = np.where(u_scale > np.finfo('float32').resolution, u_scale, 1)    \n",
    "    u_scale_reciprocal = np.reciprocal(u_scale)    \n",
    "    data_u = u_scale_reciprocal*(orig_data_u-u_ref)\n",
    "elif option==3: # option 3: no scale applied\n",
    "    print(\"option {}: no scaling\".format(option))\n",
    "    u_ref = np.zeros(orig_data_u.shape[1],dtype='float32')\n",
    "    u_scale = np.ones(orig_data_u.shape[1],dtype='float32')   \n",
    "    u_scale_reciprocal = u_scale\n",
    "    data_u = u_scale_reciprocal*(orig_data_u-u_ref)\n",
    "else:\n",
    "    raise NameError('{} is given for option, but it must be either 1, 2 or 3'.format(option))\n",
    "\n",
    "# check shapes of snapshot\n",
    "print('data shape \\t {}'.format(data_u.shape))\n",
    "print('data dtype \\t {}'.format(data_u.dtype))\n",
    "\n",
    "# restore data\n",
    "rest_data_u = u_ref + u_scale*data_u\n",
    "\n",
    "# check precision\n",
    "print('max. abs difference \\t {}'.format(np.max(np.abs(orig_data_u-rest_data_u))))\n",
    "\n",
    "# define testset and trainset indices\n",
    "nset = len(training_params)\n",
    "test_ind = np.array([],dtype='int')\n",
    "for foo in range(nset):\n",
    "    np.random.seed(foo)\n",
    "    rand_perm = np.sort(np.random.permutation(nt+1)[:int(r_test*(nt+1))])\n",
    "    rand_ind = np.arange(foo*(nt+1),(foo+1)*(nt+1))[rand_perm]\n",
    "    test_ind = np.append(test_ind,rand_ind)\n",
    "test_ind.sort()\n",
    "train_ind = np.setdiff1d(np.arange(ndata),test_ind)\n",
    "\n",
    "# set trainset and testset\n",
    "# np.random.seed(0)\n",
    "np.random.shuffle(train_ind)\n",
    "np.random.shuffle(test_ind)\n",
    "trainset = data_u[train_ind]\n",
    "testset = data_u[test_ind] \n",
    "\n",
    "# set dataset\n",
    "dataset = {'train':data_utils.TensorDataset(torch.tensor(trainset,dtype=torch.float32)),\n",
    "           'test':data_utils.TensorDataset(torch.tensor(testset,dtype=torch.float32))}\n",
    "\n",
    "# compute dataset shapes\n",
    "dataset_shapes = {'train':trainset.shape,'test':testset.shape}\n",
    "print(\"train set size: {} \\t test set size: {}\".format(dataset_shapes['train'],dataset_shapes['test']))\n",
    "\n",
    "# set data loaders\n",
    "train_loader = DataLoader(dataset=dataset['train'],batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(dataset=dataset['test'],batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "data_loaders = {'train':train_loader, 'test':test_loader}\n",
    "\n",
    "# data size\n",
    "data_size=np.prod(orig_data_u.shape)\n",
    "print('Data size:{:.8e}({:.4}GB)'.format(data_size,data_size*4/2**30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_u.max(), data_u.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_2d(m,b,db):\n",
    "    \n",
    "    # local\n",
    "    Mb=sp.diags([np.ones(nx),np.ones(nx),np.ones(nx)],[0,-1,1],(nx,nx))\n",
    "    M=sp.kron(sp.eye(ny),Mb,format=\"csr\")\n",
    "\n",
    "    Ib=sp.eye(nx)\n",
    "    N=sp.kron(sp.diags([np.ones(ny),np.ones(ny),np.ones(ny)],[0,-1,1],(ny,ny)),Ib,format=\"csr\")\n",
    "\n",
    "    local=(M+N).astype('int8')\n",
    "    I,J,V=sp.find(local)\n",
    "    local[I,J]=1\n",
    "    \n",
    "    # basis\n",
    "    M2 = int(b + db*(m-1))\n",
    "    basis = np.zeros((m,M2),dtype='int8')\n",
    "\n",
    "    block = np.ones(b,dtype='int8')\n",
    "    ind = np.arange(b)\n",
    "    for row in range(m):\n",
    "        col = ind + row*db\n",
    "        basis[row,col] = block\n",
    "    \n",
    "    # mask\n",
    "    col_ind=np.array([],dtype='int8')\n",
    "    row_ind=np.array([],dtype='int8')\n",
    "    for i in range(m):\n",
    "        col=basis[sp.find(local[i])[1]].sum(axis=0).nonzero()[0]\n",
    "        row=i*np.ones(col.size).astype(int)\n",
    "\n",
    "        col_ind=np.append(col_ind,col)\n",
    "        row_ind=np.append(row_ind,row)\n",
    "\n",
    "    connection=np.vstack((row_ind,col_ind))\n",
    "\n",
    "    # data=np.ones(row_ind.size,dtype='int8')\n",
    "    # mask=sp.csr_matrix((data,(row_ind,col_ind)),shape=(m,M2)).toarray()\n",
    "    \n",
    "    print(\"Sparsity in {} by {} mask: {:.2f}%\".format(m, M2, (1.0-connection.shape[1]/(m*M2))*100))\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.spy(mask)\n",
    "#     plt.show()\n",
    "\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# sparsity and shape of mask\n",
    "# mask_2d=create_mask_2d(m,b,db)\n",
    "connection=torch.tensor(create_mask_2d(m,b,db))\n",
    "\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "\n",
    "    # Set file names\n",
    "    file_name_AE=\"./diffusion_model/ex16_AE_\"+str(f)+\"_swish_seed_\"+str(seed)+\".p\"\n",
    "    file_name_AE_temp=\"./diffusion_temp/ex16_AE_\"+str(f)+\"_swish_temp_seed_\"+str(seed)+\".pkl\"\n",
    "    PATH = './diffusion_temp/ex16_AE_'+str(f)+\"_swish_checkpoint_seed_\"+str(seed)+\".tar\"\n",
    "    loss_hist_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".p\"\n",
    "    loss_fig_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".png\"\n",
    "    rel_recon_fig_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_rel_recon_seed_\"+str(seed)+\".png\"\n",
    "    avg_rel_err_fig_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_avg_rel_err_seed_\"+str(seed)+\".png\"\n",
    "    max_rel_err_fig_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_max_rel_err_seed_\"+str(seed)+\".png\"\n",
    "    rel_err_fig_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_rel_err_seed_\"+str(seed)+\".png\"\n",
    "\n",
    "    print(file_name_AE)\n",
    "    print(file_name_AE_temp)\n",
    "    print(PATH)\n",
    "    print(loss_fig_name)\n",
    "    print(rel_recon_fig_name)\n",
    "    print(avg_rel_err_fig_name)\n",
    "    print(max_rel_err_fig_name)\n",
    "    print(rel_err_fig_name)\n",
    "    \n",
    "    if activation=='sigmoid':\n",
    "        print('Activation function: sigmoid')\n",
    "        class Encoder(nn.Module):\n",
    "            def __init__(self,m,M1,f):\n",
    "                super(Encoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(m,M1),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.Linear(M1,f,bias=False)\n",
    "                )\n",
    "\n",
    "            def forward(self, y):     \n",
    "                y = y.view(-1,m)\n",
    "                T = self.full(y)\n",
    "                T = T.squeeze()\n",
    "\n",
    "                return T\n",
    "\n",
    "        class Decoder(nn.Module):\n",
    "            def __init__(self,f,M2,m):\n",
    "                super(Decoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(f,M2),\n",
    "                    nn.Sigmoid(),\n",
    "                    # nn.Linear(M2,m,bias=False)\n",
    "                    sl.SparseLinear(M2,m,bias=False,connectivity=connection)\n",
    "                )\n",
    "\n",
    "            def forward(self,T):\n",
    "                T = T.view(-1,f)\n",
    "                y = self.full(T)\n",
    "                y = y.squeeze()\n",
    "\n",
    "                return y\n",
    "\n",
    "    elif activation=='swish':\n",
    "        print('Activation function: swish')\n",
    "        def silu(input):\n",
    "            return input * torch.sigmoid(input)\n",
    "\n",
    "        class SiLU(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "            def forward(self, input):\n",
    "                return silu(input)\n",
    "\n",
    "        class Encoder(nn.Module):\n",
    "            def __init__(self,m,M1,f):\n",
    "                super(Encoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(m,M1),\n",
    "                    SiLU(),\n",
    "                    nn.Linear(M1,f,bias=False)\n",
    "                )\n",
    "\n",
    "            def forward(self, y):     \n",
    "                y = y.view(-1,m)\n",
    "                T = self.full(y)\n",
    "                T = T.squeeze()\n",
    "\n",
    "                return T\n",
    "\n",
    "        class Decoder(nn.Module):\n",
    "            def __init__(self,f,M2,m):\n",
    "                super(Decoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(f,M2),\n",
    "                    SiLU(),\n",
    "                    # nn.Linear(M2,m,bias=False)\n",
    "                    sl.SparseLinear(M2,m,bias=False,connectivity=connection)\n",
    "                )\n",
    "\n",
    "            def forward(self,T):\n",
    "                T = T.view(-1,f)\n",
    "                y = self.full(T)\n",
    "                y = y.squeeze()\n",
    "\n",
    "                return y\n",
    "    else:\n",
    "        raise NameError('{} is given for option, but it must be either sigmoid or swish'.format(activation))\n",
    "        \n",
    "        \n",
    "    # number of parameters and memory\n",
    "    en_para=m*M1+M1+M1*f\n",
    "    # de_para=f*M2+M2+np.count_nonzero(mask_2d)\n",
    "    de_para=f*M2+M2+connection.shape[1]\n",
    "    # print('Encoder parameters:{:.8e}({:.4}GB)'.format(en_para,en_para*4/2**30),\\\n",
    "    #       'Decoder parameters:{:.8e}({:.4}GB)'.format(de_para,(f*M2+M2+M2*m)*4/2**30))\n",
    "    print('Encoder parameters:{:.8e}({:.4}GB)'.format(en_para,en_para*4/2**30),\\\n",
    "          'Decoder parameters:{:.8e}({:.4}GB)'.format(de_para,de_para*4/2**30))\n",
    "    \n",
    "    # load model\n",
    "    try:\n",
    "        checkpoint = torch.load(PATH, map_location=device)\n",
    "\n",
    "        encoder = Encoder(m,M1,f).to(device)\n",
    "        decoder = Decoder(f,M2,m).to(device)\n",
    "\n",
    "        # # Prune\n",
    "        # prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask_2d).to(device))    \n",
    "\n",
    "        optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=l_rate)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=num_patience) \n",
    "\n",
    "        loss_func = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        last_epoch = checkpoint['epoch']\n",
    "        loss_hist = checkpoint['loss_hist']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        early_stop_counter = checkpoint['early_stop_counter']\n",
    "        best_encoder_wts = checkpoint['best_encoder_wts']\n",
    "        best_decoder_wts = checkpoint['best_decoder_wts']\n",
    "\n",
    "        print(\"\\n--------checkpoint restored--------\\n\")\n",
    "\n",
    "        # resume training\n",
    "        print(\"\")\n",
    "        print('Re-start {}th training... m={}, f={},a={}, b={}, db={}'.format(last_epoch+1, m, f, a, b, db))\n",
    "    except:\n",
    "        encoder = Encoder(m,M1,f).to(device)\n",
    "        decoder = Decoder(f,M2,m).to(device)\n",
    "\n",
    "        # # Prune\n",
    "        # prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask_2d).to(device))\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=l_rate)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=num_patience) \n",
    "\n",
    "        loss_func = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        last_epoch = 0\n",
    "        loss_hist = {'train':[],'test':[]}\n",
    "        best_loss = float(\"inf\")\n",
    "        early_stop_counter = 1\n",
    "        best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "        best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "\n",
    "        print(\"\\n--------checkpoint not restored--------\\n\")\n",
    "\n",
    "        # start training\n",
    "        print(\"\")\n",
    "        print('Start first training... m={}, f={}, a={}, b={}, db={}'.format(m, f, a, b, db))\n",
    "    pass\n",
    "\n",
    "    # train model\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(last_epoch+1,num_epochs+1):   \n",
    "\n",
    "        if epoch%num_epochs_print == 0:\n",
    "            print()\n",
    "            print('Epoch {}/{}, Learning rate {}'.format(\n",
    "                epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and test phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                encoder.train()  # Set model to training mode\n",
    "                decoder.train()  # Set model to training mode\n",
    "            else:\n",
    "                encoder.eval()   # Set model to evaluation mode\n",
    "                decoder.eval()   # Set model to evaluation mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data\n",
    "            for data, in data_loaders[phase]:\n",
    "                inputs = data.to(device)\n",
    "                targets = data.to(device)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = decoder(encoder(inputs))\n",
    "                    loss = loss_func(outputs, targets)\n",
    "\n",
    "                    # backward\n",
    "                    loss.backward()\n",
    "\n",
    "                    # optimize\n",
    "                    optimizer.step()  \n",
    "\n",
    "                    # add running loss\n",
    "                    running_loss += loss.item()*inputs.shape[0]\n",
    "                else:\n",
    "                    with torch.set_grad_enabled(False):\n",
    "                        outputs = decoder(encoder(inputs))\n",
    "                        running_loss += loss_func(outputs,targets).item()*inputs.shape[0]\n",
    "\n",
    "            # compute epoch loss\n",
    "            epoch_loss = running_loss / dataset_shapes[phase][0]\n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "\n",
    "            # update learning rate\n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            if epoch%num_epochs_print == 0:\n",
    "                print('{} MSELoss: {}'.format(\n",
    "                    phase, epoch_loss))\n",
    "\n",
    "        # deep copy the model\n",
    "        if round(loss_hist['test'][-1],10) < round(best_loss,10):\n",
    "            best_loss = loss_hist['test'][-1]\n",
    "            early_stop_counter = 1\n",
    "            best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "            best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= early_stop_patience:  \n",
    "                break\n",
    "\n",
    "        # save checkpoint every num_epoch_print\n",
    "        if epoch%num_epochs_print== 0:\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'encoder_state_dict': encoder.state_dict(),\n",
    "                        'decoder_state_dict': decoder.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss_hist': loss_hist,\n",
    "                        'best_loss': best_loss,\n",
    "                        'early_stop_counter': early_stop_counter,\n",
    "                        'best_encoder_wts': best_encoder_wts,\n",
    "                        'best_decoder_wts': best_decoder_wts,\n",
    "                        }, PATH)        \n",
    "\n",
    "    print()\n",
    "    print('Epoch {}/{}, Learning rate {}'.format(epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    print('-' * 10)\n",
    "    print('train MSELoss: {}'.format(loss_hist['train'][-1]))\n",
    "    print('test MSELoss: {}'.format(loss_hist['test'][-1]))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    # load best model weights\n",
    "    encoder.load_state_dict(best_encoder_wts)\n",
    "    decoder.load_state_dict(best_decoder_wts)\n",
    "\n",
    "    # compute best train MSELoss\n",
    "    encoder.to('cpu').eval()\n",
    "    decoder.to('cpu').eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_inputs = torch.tensor(data_u[train_ind])\n",
    "        train_targets = torch.tensor(data_u[train_ind])\n",
    "        train_outputs = decoder(encoder(train_inputs))\n",
    "        train_loss = loss_func(train_outputs,train_targets).item()\n",
    "\n",
    "    # print out training time and best results\n",
    "    print()\n",
    "    if epoch < num_epochs:\n",
    "        print('Early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'\\\n",
    "              .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    else:\n",
    "        print('No early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'\\\n",
    "              .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print('-' * 10)\n",
    "    print('Best train MSELoss: {}'.format(train_loss))\n",
    "    print('Best test MSELoss: {}'.format(best_loss))\n",
    "\n",
    "    # save models\n",
    "    print()\n",
    "    print(\"Saving after {}th training to\".format(epoch),file_name_AE_temp)\n",
    "    torch.save((encoder,decoder),file_name_AE_temp)\n",
    "    \n",
    "    # delete checkpoint\n",
    "    try:\n",
    "        os.remove(PATH)\n",
    "        print()\n",
    "        print(\"checkpoint removed\")\n",
    "    except:\n",
    "        print(\"no checkpoint exists\") \n",
    "\n",
    "    # release gpu memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # plot train and test loss\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.semilogy(loss_hist['train'])\n",
    "    plt.semilogy(loss_hist['test'])\n",
    "    plt.legend(['train','test'])\n",
    "    # plt.show()   \n",
    "    plt.savefig(loss_fig_name)\n",
    "    \n",
    "    # load models\n",
    "    try:\n",
    "        encoder_u,decoder_u = torch.load(file_name_AE_temp,map_location='cpu')\n",
    "        print(\"\\n--------model restored--------\\n\")\n",
    "    except:\n",
    "        print(\"\\n--------model not restored--------\\n\")\n",
    "        pass\n",
    "\n",
    "    # load weights and bias\n",
    "    en_wu1_s=encoder_u.full[0].weight.detach().numpy().astype('float32')\n",
    "    en_bu1=encoder_u.full[0].bias.detach().numpy().astype('float32')\n",
    "    en_wu2=encoder_u.full[2].weight.detach().numpy().astype('float32')\n",
    "    de_wu1=decoder_u.full[0].weight.detach().numpy().astype('float32')\n",
    "    de_bu1=decoder_u.full[0].bias.detach().numpy().astype('float32')\n",
    "    # de_wu2_s=decoder_u.full[2].weight.detach().numpy().astype('float32')\n",
    "    # de_wu2_s_sp=sp.csr_matrix(de_wu2_s,dtype='float32')\n",
    "    aaa=decoder.full[2].weight.detach()\n",
    "    de_wu2_s_sp=sp.csr_matrix((aaa.values(),(aaa.indices()[0],aaa.indices()[1])),\n",
    "                            shape=aaa.size(),dtype='float32')\n",
    "    de_wu2_s=de_wu2_s_sp.toarray().astype('float32')\n",
    "\n",
    "    # rescale weights\n",
    "    en_wu1=en_wu1_s*u_scale_reciprocal\n",
    "    de_wu1T=de_wu1.T\n",
    "    de_wu2T=u_scale*de_wu2_s.T\n",
    "    de_wu2=de_wu2T.T\n",
    "    de_wu2_sp=sp.csr_matrix(de_wu2,dtype='float32')\n",
    "    de_wu2T_sp=de_wu2_sp.T\n",
    "    \n",
    "    # save weights and references   \n",
    "    AE={'en_wu1':en_wu1,'en_bu1':en_bu1,'en_wu2':en_wu2,\n",
    "        'de_wu1':de_wu1,'de_bu1':de_bu1,'de_wu2':de_wu2,\n",
    "        'de_wu1T':de_wu1T,'de_wu2T':de_wu2T,'de_wu2_sp':de_wu2_sp,'de_wu2T_sp':de_wu2T_sp,'u_ref':u_ref,}\n",
    "\n",
    "    with open(file_name_AE,'wb') as ffff:\n",
    "        pickle.dump(AE,ffff)\n",
    "        \n",
    "    # save loss history\n",
    "    with open(loss_hist_name,'wb') as f5:\n",
    "        pickle.dump(loss_hist,f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete checkpoint\n",
    "try:\n",
    "    os.remove(PATH)\n",
    "    print()\n",
    "    print(\"checkpoint removed\")\n",
    "except:\n",
    "    print(\"no checkpoint exists\") \n",
    "\n",
    "# release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# plot train and test loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.semilogy(loss_hist['train'],linestyle='dotted',linewidth=2)\n",
    "plt.semilogy(loss_hist['test'],linestyle='dashdot',linewidth=2)\n",
    "plt.legend(['train','test'],fontsize=24)\n",
    "plt.xlabel(\"Epochs\",fontsize=28)\n",
    "plt.ylabel(\"MSE\",fontsize=28)\n",
    "# plt.show()   \n",
    "plt.savefig(loss_fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "    \n",
    "    loss_hist_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".p\"\n",
    "    loss_fig_name=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".png\"\n",
    "    \n",
    "    with open(loss_hist_name,'rb') as f6:\n",
    "        loss_hist_name=pickle.load(f6)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.semilogy(loss_hist['train'],linestyle='dotted',linewidth=2)\n",
    "    plt.semilogy(loss_hist['test'],linestyle='dashdot',linewidth=2)\n",
    "    plt.legend(['train','test'],fontsize=24)\n",
    "    plt.xlabel(\"Epochs\",fontsize=28)\n",
    "    plt.ylabel(\"MSE\",fontsize=28)\n",
    "    # plt.show()   \n",
    "    plt.savefig(loss_fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params = np.arange(80,121,5)\n",
    "\n",
    "# load snapshot\n",
    "snapshot = np.array([])\n",
    "for i in training_params:\n",
    "    ex = np.load('./diffusion_data/ex16_interp_{}.npz'.format(i), allow_pickle = True)\n",
    "    ex = ex.f.arr_0\n",
    "    snapshot = np.append(snapshot, ex)\n",
    "snapshot = snapshot.reshape(len(training_params)*(nt+1),-1).astype('float32')\n",
    "\n",
    "# number of data points\n",
    "ndata = snapshot.shape[0]\n",
    "\n",
    "orig_data_u=np.copy(snapshot)\n",
    "\n",
    "# f_list=np.array([3,4,5,6])\n",
    "print(\"Latent Sapce Dim.: {}\".format(f_list))\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "\n",
    "    # Set file names\n",
    "    file_name_AE=\"./diffusion_model/ex16_AE_\"+str(f)+\"_swish_seed_\"+str(seed)+\".p\"\n",
    "    print(file_name_AE)\n",
    "\n",
    "    with open(file_name_AE,'rb') as fff:\n",
    "        AE = pickle.load(fff)\n",
    "\n",
    "    en_wu1=AE['en_wu1']\n",
    "    en_bu1=AE['en_bu1']\n",
    "    en_wu2=AE['en_wu2']\n",
    "    de_wu1=AE['de_wu1']\n",
    "    de_bu1=AE['de_bu1']\n",
    "    de_wu2=AE['de_wu2']\n",
    "    de_wu1T=AE['de_wu1T']\n",
    "    de_wu2T=AE['de_wu2T']\n",
    "    de_wu2_sp=AE['de_wu2_sp']\n",
    "    de_wu2T_sp=AE['de_wu2T_sp']\n",
    "    u_ref=AE['u_ref']\n",
    "\n",
    "    latent_dim=de_wu1.shape[1]\n",
    "\n",
    "    # numpy version of AE\n",
    "    def sigmoid_np(input):\n",
    "        return (1.0/(1.0+np.exp(-input))).astype('float32')\n",
    "\n",
    "    def encoder_u_np_forward(x):\n",
    "        z1 = en_wu1.dot(x) + en_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = en_wu2.dot(a1)   \n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = de_wu2.dot(a1)  \n",
    "        return y\n",
    "\n",
    "    def decoder_u_sp_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = de_wu2.dot(a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = dout.dot(de_wu2T)   \n",
    "        return y,dydxT\n",
    "\n",
    "    def decoder_u_sp_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = sp.csr_matrix.dot(dout,de_wu2T_sp)\n",
    "        return y,dydxT\n",
    "\n",
    "    # compute MSELoss\n",
    "    comp_orig_data_u=np.zeros((ndata,f))\n",
    "    rest_orig_data_u=np.zeros(orig_data_u.shape)\n",
    "\n",
    "    for k in range(ndata):\n",
    "        comp_orig_data_u[k]=encoder_u_np_forward(orig_data_u[k]-u_ref)\n",
    "        rest_orig_data_u[k]=decoder_u_sp_forward(comp_orig_data_u[k]) + u_ref\n",
    "\n",
    "    print(\"MSELoss of AE (rescaled): {:.8e}\".format(\n",
    "        np.linalg.norm(orig_data_u-rest_orig_data_u)**2/np.prod(orig_data_u.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection Error and Residual Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_list=np.array([3,4,5,6]); seed=5\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "\n",
    "    # Set file names\n",
    "    file_name_AE=\"./diffusion_model/ex16_AE_\"+str(f)+\"_swish_seed_\"+str(seed)+\".p\"\n",
    "    file_path_prj_result = \"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_proj_result_seed_\"+str(seed)+\".p\"\n",
    "    file_path_residual=\"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_residual_seed_\"+str(seed)+\".p\"\n",
    "    file_path_residual_SVD=\"./diffusion_model/ex16_AE_\"+str(f)+\"_swish_residual_SVD_seed_\"+str(seed)+\".p\"\n",
    "\n",
    "    print(file_name_AE)\n",
    "    print(file_path_prj_result)\n",
    "    print(file_path_residual)\n",
    "    print(file_path_residual_SVD)\n",
    "    \n",
    "    with open(file_name_AE,'rb') as fff:\n",
    "        AE = pickle.load(fff)\n",
    "\n",
    "    en_wu1=AE['en_wu1']\n",
    "    en_bu1=AE['en_bu1']\n",
    "    en_wu2=AE['en_wu2']\n",
    "    de_wu1=AE['de_wu1']\n",
    "    de_bu1=AE['de_bu1']\n",
    "    de_wu2=AE['de_wu2']\n",
    "    de_wu1T=AE['de_wu1T']\n",
    "    de_wu2T=AE['de_wu2T']\n",
    "    de_wu2_sp=AE['de_wu2_sp']\n",
    "    de_wu2T_sp=AE['de_wu2T_sp']\n",
    "    u_ref=AE['u_ref']\n",
    "\n",
    "    latent_dim=de_wu1.shape[1]\n",
    "\n",
    "    # numpy version of AE\n",
    "    def sigmoid_np(input):\n",
    "        return (1.0/(1.0+np.exp(-input))).astype('float32')\n",
    "\n",
    "    def encoder_u_np_forward(x):\n",
    "        z1 = en_wu1.dot(x) + en_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = en_wu2.dot(a1)   \n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = de_wu2.dot(a1)  \n",
    "        return y\n",
    "\n",
    "    def decoder_u_sp_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = de_wu2.dot(a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = dout.dot(de_wu2T)   \n",
    "        return y,dydxT\n",
    "\n",
    "    def decoder_u_sp_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = sp.csr_matrix.dot(dout,de_wu2T_sp)\n",
    "        return y,dydxT\n",
    "    \n",
    "    results={'avg_rel_err':[],'rel_err':[],'elapsed_time':[]}\n",
    "    residual_snapshot=[]\n",
    "\n",
    "    for FOM_parameter in training_params:\n",
    "        # Load FOM solution\n",
    "        ex = np.load('./diffusion_data/ex16_interp_{}.npz'.format(FOM_parameter), allow_pickle = True)\n",
    "        ex = ex.f.arr_0\n",
    "        u_full = ex.reshape(nt+1,-1).astype('float32')\n",
    "\n",
    "        # take measurments\n",
    "        um = u_full[:,msmt_idx]\n",
    "\n",
    "        # Initial condition\n",
    "        u0=u_full[0]\n",
    "        u_hat0=encoder_u_np_forward(u0.astype('float32')-u_ref)\n",
    "\n",
    "        # gappyAE\n",
    "        t_start_gappyAE=time.time()\n",
    "\n",
    "        # solution\n",
    "        u_reduced=np.zeros((nt+1,latent_dim))\n",
    "        u_gappyAE=np.zeros((nt+1,ny*nx))\n",
    "\n",
    "        # IC\n",
    "        u_reduced[0]=np.copy(u_hat0)\n",
    "        u_gappyAE[0]=np.copy(u0)\n",
    "\n",
    "        residual=u_full[0]-(u_ref+decoder_u_sp_forward(u_hat0))\n",
    "        residual_snapshot.append(residual)\n",
    "        for k in range(nt):\n",
    "            print(\"\")\n",
    "            print(k,\"th time step:\")\n",
    "\n",
    "            u_hatw=np.copy(u_reduced[k])\n",
    "\n",
    "            umw,Jg_umT=decoder_u_sp_forward_backwardT(u_hatw)\n",
    "\n",
    "            umw += u_ref[msmt_idx]\n",
    "\n",
    "            Jg_um_pinv=np.linalg.pinv(Jg_umT.T)\n",
    "\n",
    "            r_um_hat = um[k+1]-umw\n",
    "\n",
    "            res = np.linalg.norm(Jg_umT.dot(r_um_hat))\n",
    "            res_hist=[res]\n",
    "            residual=u_full[k+1]-(u_ref+decoder_u_sp_forward(u_hatw))\n",
    "            residual_snapshot.append(residual)\n",
    "            for itr in range(maxitr):\n",
    "                du_hatw = Jg_um_pinv.dot(r_um_hat)\n",
    "\n",
    "                u_hatw += du_hatw\n",
    "\n",
    "                umw,Jg_umT=decoder_u_sp_forward_backwardT(u_hatw)\n",
    "\n",
    "                umw += u_ref[msmt_idx]\n",
    "\n",
    "                Jg_um_pinv=np.linalg.pinv(Jg_umT.T)\n",
    "\n",
    "                r_um_hat = um[k+1]-umw\n",
    "\n",
    "                res = np.linalg.norm(Jg_umT.dot(r_um_hat))\n",
    "                res_hist.append(res)\n",
    "                residual=u_full[k+1]-(u_ref+decoder_u_sp_forward(u_hatw))\n",
    "                residual_snapshot.append(residual)\n",
    "                print(itr,\"th Newton iteration\", \"res:\", \"{:.8e}\".format(np.linalg.norm(residual)))\n",
    "\n",
    "                if res<tol:\n",
    "                    break\n",
    "\n",
    "            u_reduced[k+1]=u_hatw.copy()\n",
    "            u_gappyAE[k+1]=u_ref+decoder_u_sp_forward(u_reduced[k+1])\n",
    "\n",
    "        # elapsed time\n",
    "        t_elapsed_gappyAE=time.time()-t_start_gappyAE\n",
    "        print()\n",
    "        print('Time elapsed: {} sec'.format(t_elapsed_gappyAE))\n",
    "\n",
    "        # error\n",
    "        u_rel_err_gappyAE=np.linalg.norm(u_full-u_gappyAE,ord=2,axis=1)/np.linalg.norm(u_full,ord=2,axis=1)*100\n",
    "        u_avg_rel_err=np.sqrt(np.sum(np.linalg.norm(u_full-u_gappyAE,ord=2,axis=1)**2))/np.sqrt(np.sum(np.linalg.norm(u_full,ord=2,axis=1)**2))*100\n",
    "\n",
    "        print(\"average relative error of u: {}%\".format(u_avg_rel_err))\n",
    "        print(\"maximum relative error of u: {}%\".format(np.max(u_rel_err_gappyAE)))\n",
    "\n",
    "        # save result\n",
    "        results['avg_rel_err'].append(u_avg_rel_err)\n",
    "        results['rel_err'].append(u_rel_err_gappyAE)\n",
    "        results['elapsed_time'].append(t_elapsed_gappyAE)\n",
    "\n",
    "    results['avg_rel_err']=np.array(results['avg_rel_err'])\n",
    "    results['rel_err']=np.array(results['rel_err'])\n",
    "    results['elapsed_time']=np.array(results['elapsed_time'])\n",
    "\n",
    "    with open(file=file_path_prj_result, mode='wb') as ff:\n",
    "        pickle.dump(results, ff)\n",
    "        print(file_path_prj_result)\n",
    "\n",
    "    residual_snapshot=np.array(residual_snapshot)    \n",
    "    with open(file=file_path_residual, mode='wb') as fff:\n",
    "        pickle.dump(residual_snapshot, fff)\n",
    "        print(file_path_residual)\n",
    "\n",
    "    # do svd decomposition\n",
    "    U,S,VT=np.linalg.svd(residual_snapshot.T,full_matrices=False)\n",
    "\n",
    "    SVD={'U':U,'S':S,'VT':VT}\n",
    "    with open(file=file_path_residual_SVD, mode='wb') as fff:\n",
    "        pickle.dump(SVD,fff)  \n",
    "        print(file_path_residual_SVD)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot other cases together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=20)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=24)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=24)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=20)    # legend fontsize\n",
    "plt.rc('figure', titlesize=28)  # fontsize of the figure title\n",
    "\n",
    "linestyle=['solid','dotted','dashed','dashdot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list=[3,4,5,6]\n",
    "\n",
    "plt.figure(num=1,figsize=(10,6))\n",
    "plt.figure(num=2,figsize=(10,6))\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i]\n",
    "    file_path_prj_result = \"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_proj_result_seed_\"+str(seed)+\".p\"\n",
    "    with open(file=file_path_prj_result, mode='rb') as ff:\n",
    "        results=pickle.load(ff)   \n",
    "    plt.figure(num=1)\n",
    "    plt.plot(training_params/100,results['avg_rel_err'],linestyle=linestyle[i],linewidth=4)\n",
    "\n",
    "    plt.figure(num=2)\n",
    "    plt.plot(training_params/100,results['rel_err'].max(axis=1),linestyle=linestyle[i],linewidth=4)\n",
    "\n",
    "plt.figure(num=1)    \n",
    "plt.xlabel('FOM parameter')\n",
    "plt.ylabel('Avg. Rel. Err.')\n",
    "# plt.ylim([0.03418848579768803,0.09671082491576243])\n",
    "plt.legend([\"Latent Space Dim. \"+str(f) for f in f_list])\n",
    "plt.savefig(\"./diffusion_result/ex16_AE_swish_proj_avg_rel_err_seed_\"+str(seed)+\".png\")\n",
    "# print(plt.gca().get_ylim()); (0.03418848579768803, 0.09073068579684458)\n",
    "\n",
    "plt.figure(num=2)\n",
    "plt.xlabel('FOM parameter')\n",
    "plt.ylabel('Max. Rel. Err.')\n",
    "# plt.ylim([0.180692401599163,0.6269599283378853])\n",
    "plt.legend([\"Latent Space Dim. \"+str(f) for f in f_list])\n",
    "plt.savefig(\"./diffusion_result/ex16_AE_swish_proj_max_rel_err_seed_\"+str(seed)+\".png\")\n",
    "# print(plt.gca().get_ylim()); (0.180692401599163, 0.6264354403451524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_list)\n",
    "print(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=2; j=-1        \n",
    "# f=f_list[i]\n",
    "# param=training_params[j]\n",
    "\n",
    "# file_path_prj_result = \"./diffusion_result/ex16_AE_\"+str(f)+\"_swish_proj_result_seed_\"+str(seed)+\".p\"\n",
    "# with open(file=file_path_prj_result, mode='rb') as ff:\n",
    "#     results=pickle.load(ff)\n",
    "\n",
    "# u_rel_err_gappyAE=results['rel_err'][j,1:]\n",
    "# plt.figure(figsize=(10,5))\n",
    "\n",
    "# plt.plot(u_rel_err_gappyAE.flatten())\n",
    "# plt.xlabel('Data Point')\n",
    "# plt.ylabel('Relative Error (%)')\n",
    "# plt.title('Proj. Err. Latent Space Dim.={}, Param = {}'.format(f,param))\n",
    "# plt.show()\n",
    "\n",
    "# # plot original data\n",
    "# vmin=0; vmax=1\n",
    "\n",
    "# # AE\n",
    "# plt.figure(figsize=(20,4))\n",
    "\n",
    "# plt.subplot(1,5,1)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[0].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t=0')\n",
    "\n",
    "# plt.subplot(1,5,2)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[int(nt/4)].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*int(nt/4)))\n",
    "\n",
    "# plt.subplot(1,5,3)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[int(nt/4)*2].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*int(nt/4)*2))\n",
    "\n",
    "# plt.subplot(1,5,4)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[int(nt/4)*3].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*int(nt/4)*3))\n",
    "\n",
    "# plt.subplot(1,5,5)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[nt].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*nt))\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # FOM\n",
    "# plt.figure(figsize=(20,4))\n",
    "\n",
    "# plt.subplot(1,5,1)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[0].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t=0')\n",
    "\n",
    "# plt.subplot(1,5,2)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[int(nt/4)].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*int(nt/4)))\n",
    "\n",
    "# plt.subplot(1,5,3)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[int(nt/4)*2].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*int(nt/4)*2))\n",
    "\n",
    "# plt.subplot(1,5,4)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[int(nt/4)*3].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*int(nt/4)*3))\n",
    "\n",
    "# plt.subplot(1,5,5)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[nt].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*nt))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
