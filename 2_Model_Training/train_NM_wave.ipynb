{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import lr_scheduler\n",
    "import sparselinear as sl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse as sp\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "import sys,time\n",
    "import os\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directories = [\"../__model/ex9_wave\", \"../__result/ex9_wave\", \"../__temp/ex9_wave\"]\n",
    "\n",
    "for dir_name in directories:\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Set device\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(\"Using device:\", device, '\\n')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt=500\n",
      "training parameters=[ 80  85  90  95 100 105 110 115 120]\n"
     ]
    }
   ],
   "source": [
    "# Given parameters\n",
    "nx = 60\n",
    "ny = 60\n",
    "m=nx*ny\n",
    "tf = 5.0\n",
    "dt = 1.0e-2\n",
    "nt = int(tf/dt)\n",
    "xmin = 0; xmax = 1\n",
    "ymin = 0; ymax = 1\n",
    "dims = (ny,nx)\n",
    "\n",
    "# set batch_size, number of epochs, paitience for early stop\n",
    "batch_size = 240\n",
    "num_epochs = 10000\n",
    "num_epochs_print = 100\n",
    "early_stop_patience = 200\n",
    "num_patience = 50\n",
    "l_rate = 0.001\n",
    "\n",
    "# Choose data normalize option (option 1: -1<=X<=1 option 2: 0<=X<=1, option 3: no scaling)\n",
    "option = 3\n",
    "\n",
    "# Choose activation function (sigmoid, swish)\n",
    "activation = 'swish'\n",
    "\n",
    "# parameters\n",
    "training_params = np.arange(80,121,5)\n",
    "print(\"nt={}\".format(nt))\n",
    "print(\"training parameters={}\".format(training_params))\n",
    "\n",
    "# train:test split\n",
    "r_test=0.2\n",
    "\n",
    "# set the number of nodes in each layer\n",
    "a = 2\n",
    "b = int(100)\n",
    "db = int(10)\n",
    "\n",
    "M1 = int(a*m) # encoder hidden layer\n",
    "M2 = b + (m-1)*db # decoder hidden layer\n",
    "\n",
    "# latent space dimension\n",
    "f_list=np.array([3,4,5,6])\n",
    "\n",
    "# generate mesh grid\n",
    "[xv,yv]=np.meshgrid(np.linspace(xmin,xmax,nx),np.linspace(ymin,ymax,ny),indexing='xy')\n",
    "x=xv.flatten()\n",
    "y=yv.flatten()\n",
    "\n",
    "multi_index_i,multi_index_j=np.meshgrid(np.arange(nx),np.arange(ny),indexing='xy')\n",
    "full_multi_index=(multi_index_j.flatten(),multi_index_i.flatten())\n",
    "free_multi_index=(multi_index_j[1:-1,1:-1].flatten(),multi_index_i[1:-1,1:-1].flatten())\n",
    "x0_multi_index=(multi_index_j[1:-1,0].flatten(),multi_index_i[1:-1,0].flatten())\n",
    "x1_multi_index=(multi_index_j[1:-1,-1].flatten(),multi_index_i[1:-1,-1].flatten())\n",
    "y0_multi_index=(multi_index_j[0,1:-1].flatten(),multi_index_i[0,1:-1].flatten())\n",
    "y1_multi_index=(multi_index_j[-1,1:-1].flatten(),multi_index_i[-1,1:-1].flatten())\n",
    "\n",
    "dims=(ny,nx)\n",
    "full_raveled_indicies=np.ravel_multi_index(full_multi_index,dims)\n",
    "free_raveled_indicies=np.ravel_multi_index(free_multi_index,dims)\n",
    "x0_raveled_indicies=np.ravel_multi_index(x0_multi_index,dims)\n",
    "x1_raveled_indicies=np.ravel_multi_index(x1_multi_index,dims)\n",
    "x01_raveled_indicies=np.concatenate((x0_raveled_indicies,x1_raveled_indicies))\n",
    "y0_raveled_indicies=np.ravel_multi_index(y0_multi_index,dims)\n",
    "y1_raveled_indicies=np.ravel_multi_index(y1_multi_index,dims)\n",
    "y01_raveled_indicies=np.concatenate((y0_raveled_indicies,y1_raveled_indicies))\n",
    "fixed_raveled_indicies=np.setdiff1d(full_raveled_indicies,free_raveled_indicies)\n",
    "\n",
    "# measurements\n",
    "msmt_idx=full_raveled_indicies\n",
    "\n",
    "# Gauss-Newton iteration parameters\n",
    "maxitr=4\n",
    "tol=1e-8\n",
    "\n",
    "# random number generator seed\n",
    "seed=np.random.randint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4509, 3600) float32\n",
      "option 3: no scaling\n",
      "data shape \t (4509, 3600)\n",
      "data dtype \t float32\n",
      "max. abs difference \t 0.0\n",
      "train set size: (3609, 3600) \t test set size: (900, 3600)\n",
      "Data size:1.62324000e+07(0.06047GB)\n"
     ]
    }
   ],
   "source": [
    "# load snapshot\n",
    "snapshot = np.array([])\n",
    "for i in training_params:\n",
    "    ex = np.load('../__data/ex9_wave/ex23_interp_{}.npz'.format(i), allow_pickle = True)\n",
    "    ex = ex.f.arr_0\n",
    "    snapshot = np.append(snapshot, ex)\n",
    "snapshot = snapshot.reshape(len(training_params)*(nt+1),-1).astype('float32')\n",
    "print(snapshot.shape,snapshot.dtype)\n",
    "\n",
    "# number of data points\n",
    "ndata = snapshot.shape[0]\n",
    "\n",
    "# generate mesh grid\n",
    "[xv,yv]=np.meshgrid(np.linspace(xmin,xmax,nx),np.linspace(ymin,ymax,ny),indexing='xy')\n",
    "x=xv.flatten()\n",
    "y=yv.flatten()\n",
    "\n",
    "multi_index_i,multi_index_j=np.meshgrid(np.arange(nx),np.arange(ny),indexing='xy')\n",
    "full_multi_index=(multi_index_j.flatten(),multi_index_i.flatten())\n",
    "dims=(ny,nx)\n",
    "full_raveled_indicies=np.ravel_multi_index(full_multi_index,dims)\n",
    "\n",
    "orig_data_u=np.copy(snapshot)\n",
    "\n",
    "# normalize data\n",
    "if option==1: # option 1: -1<=X<=1\n",
    "    print(\"option {}: -1<=X<=1\".format(option))\n",
    "    u_ref = (np.max(orig_data_u,axis=0)+np.min(orig_data_u,axis=0))/2.0\n",
    "    u_scale = (np.max(orig_data_u,axis=0)-np.min(orig_data_u,axis=0))/2.0\n",
    "    u_scale = np.where(u_scale > np.finfo('float32').resolution, u_scale, 1)\n",
    "    u_scale_reciprocal = np.reciprocal(u_scale)  \n",
    "    data_u = u_scale_reciprocal*(orig_data_u-u_ref)\n",
    "    \n",
    "elif option==2: # option 2: 0<=X<=1\n",
    "    print(\"option {}: 0<=X<=1\".format(option))\n",
    "    u_ref = np.min(orig_data_u,axis=0)    \n",
    "    u_scale = np.max(orig_data_u,axis=0)-np.min(orig_data_u,axis=0)    \n",
    "    u_scale = np.where(u_scale > np.finfo('float32').resolution, u_scale, 1)    \n",
    "    u_scale_reciprocal = np.reciprocal(u_scale)    \n",
    "    data_u = u_scale_reciprocal*(orig_data_u-u_ref)\n",
    "elif option==3: # option 3: no scale applied\n",
    "    print(\"option {}: no scaling\".format(option))\n",
    "    u_ref = np.zeros(orig_data_u.shape[1],dtype='float32')\n",
    "    u_scale = np.ones(orig_data_u.shape[1],dtype='float32')   \n",
    "    u_scale_reciprocal = u_scale\n",
    "    data_u = u_scale_reciprocal*(orig_data_u-u_ref)\n",
    "else:\n",
    "    raise NameError('{} is given for option, but it must be either 1, 2 or 3'.format(option))\n",
    "\n",
    "# check shapes of snapshot\n",
    "print('data shape \\t {}'.format(data_u.shape))\n",
    "print('data dtype \\t {}'.format(data_u.dtype))\n",
    "\n",
    "# restore data\n",
    "rest_data_u = u_ref + u_scale*data_u\n",
    "\n",
    "# check precision\n",
    "print('max. abs difference \\t {}'.format(np.max(np.abs(orig_data_u-rest_data_u))))\n",
    "\n",
    "# define testset and trainset indices\n",
    "nset = len(training_params)\n",
    "test_ind = np.array([],dtype='int')\n",
    "for foo in range(nset):\n",
    "    np.random.seed(foo)\n",
    "    rand_perm = np.sort(np.random.permutation(nt+1)[:int(r_test*(nt+1))])\n",
    "    rand_ind = np.arange(foo*(nt+1),(foo+1)*(nt+1))[rand_perm]\n",
    "    test_ind = np.append(test_ind,rand_ind)\n",
    "test_ind.sort()\n",
    "train_ind = np.setdiff1d(np.arange(ndata),test_ind)\n",
    "\n",
    "# set trainset and testset\n",
    "# np.random.seed(0)\n",
    "np.random.shuffle(train_ind)\n",
    "np.random.shuffle(test_ind)\n",
    "trainset = data_u[train_ind]\n",
    "testset = data_u[test_ind] \n",
    "\n",
    "# set dataset\n",
    "dataset = {'train':data_utils.TensorDataset(torch.tensor(trainset,dtype=torch.float32)),\n",
    "           'test':data_utils.TensorDataset(torch.tensor(testset,dtype=torch.float32))}\n",
    "\n",
    "# compute dataset shapes\n",
    "dataset_shapes = {'train':trainset.shape,'test':testset.shape}\n",
    "print(\"train set size: {} \\t test set size: {}\".format(dataset_shapes['train'],dataset_shapes['test']))\n",
    "\n",
    "# set data loaders\n",
    "train_loader = DataLoader(dataset=dataset['train'],batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(dataset=dataset['test'],batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "data_loaders = {'train':train_loader, 'test':test_loader}\n",
    "\n",
    "# data size\n",
    "data_size=np.prod(orig_data_u.shape)\n",
    "print('Data size:{:.8e}({:.4}GB)'.format(data_size,data_size*4/2**30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_2d(m,b,db):\n",
    "    \n",
    "    # local\n",
    "    Mb=sp.diags([np.ones(nx),np.ones(nx),np.ones(nx)],[0,-1,1],(nx,nx))\n",
    "    M=sp.kron(sp.eye(ny),Mb,format=\"csr\")\n",
    "\n",
    "    Ib=sp.eye(nx)\n",
    "    N=sp.kron(sp.diags([np.ones(ny),np.ones(ny),np.ones(ny)],[0,-1,1],(ny,ny)),Ib,format=\"csr\")\n",
    "\n",
    "    local=(M+N).astype('int8')\n",
    "    I,J,V=sp.find(local)\n",
    "    local[I,J]=1\n",
    "    \n",
    "    # basis\n",
    "    M2 = int(b + db*(m-1))\n",
    "    basis = np.zeros((m,M2),dtype='int8')\n",
    "\n",
    "    block = np.ones(b,dtype='int8')\n",
    "    ind = np.arange(b)\n",
    "    for row in range(m):\n",
    "        col = ind + row*db\n",
    "        basis[row,col] = block\n",
    "    \n",
    "    # mask\n",
    "    col_ind=np.array([],dtype='int8')\n",
    "    row_ind=np.array([],dtype='int8')\n",
    "    for i in range(m):\n",
    "        col=basis[sp.find(local[i])[1]].sum(axis=0).nonzero()[0]\n",
    "        row=i*np.ones(col.size).astype(int)\n",
    "\n",
    "        col_ind=np.append(col_ind,col)\n",
    "        row_ind=np.append(row_ind,row)\n",
    "\n",
    "    connection=np.vstack((row_ind,col_ind))\n",
    "\n",
    "    # data=np.ones(row_ind.size,dtype='int8')\n",
    "    # mask=sp.csr_matrix((data,(row_ind,col_ind)),shape=(m,M2)).toarray()\n",
    "    \n",
    "    print(\"Sparsity in {} by {} mask: {:.2f}%\".format(m, M2, (1.0-connection.shape[1]/(m*M2))*100))\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.spy(mask)\n",
    "#     plt.show()\n",
    "\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in 3600 by 36090 mask: 99.12%\n",
      "./wave_model/ex23_AE_3_swish_seed_3.p\n",
      "./wave_temp/ex23_AE_3_swish_temp_seed_3.pkl\n",
      "./wave_temp/ex23_AE_3_swish_checkpoint_seed_3.tar\n",
      "./wave_result/ex23_AE_3_swish_loss_seed_3.png\n",
      "./wave_result/ex23_AE_3_swish_rel_recon_seed_3.png\n",
      "./wave_result/ex23_AE_3_swish_avg_rel_err_seed_3.png\n",
      "./wave_result/ex23_AE_3_swish_max_rel_err_seed_3.png\n",
      "./wave_result/ex23_AE_3_swish_rel_err_seed_3.png\n",
      "Activation fundtion: swish\n",
      "Encoder parameters:2.59488000e+07(0.09667GB) Decoder parameters:1.28316000e+06(0.00478GB)\n",
      "\n",
      "--------checkpoint not restored--------\n",
      "\n",
      "\n",
      "Start first training... m=3600, f=3, a=2, b=100, db=10\n",
      "\n",
      "Epoch 100/10000, Learning rate 0.001\n",
      "----------\n",
      "train MSELoss: 5.5233818894851774e-05\n",
      "test MSELoss: 5.592384040937759e-05\n",
      "\n",
      "Epoch 200/10000, Learning rate 0.001\n",
      "----------\n",
      "train MSELoss: 1.5627232980843773e-05\n",
      "test MSELoss: 2.2899560887405337e-05\n",
      "\n",
      "Epoch 300/10000, Learning rate 0.0001\n",
      "----------\n",
      "train MSELoss: 4.253352220628264e-06\n",
      "test MSELoss: 4.29015171903302e-06\n",
      "\n",
      "Epoch 400/10000, Learning rate 0.0001\n",
      "----------\n",
      "train MSELoss: 4.124011100751991e-06\n",
      "test MSELoss: 4.246607052967495e-06\n",
      "\n",
      "Epoch 500/10000, Learning rate 0.0001\n",
      "----------\n",
      "train MSELoss: 3.9578491623426255e-06\n",
      "test MSELoss: 4.156269793990456e-06\n",
      "\n",
      "Epoch 600/10000, Learning rate 1e-05\n",
      "----------\n",
      "train MSELoss: 3.4974180490610943e-06\n",
      "test MSELoss: 3.580390936501014e-06\n",
      "\n",
      "Epoch 700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.4587022656188995e-06\n",
      "test MSELoss: 3.563896901444726e-06\n",
      "\n",
      "Epoch 800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.4539137024909064e-06\n",
      "test MSELoss: 3.559182975247192e-06\n",
      "\n",
      "Epoch 900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.4478125485576785e-06\n",
      "test MSELoss: 3.5528591373198043e-06\n",
      "\n",
      "Epoch 1000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.441089093476367e-06\n",
      "test MSELoss: 3.546088343379476e-06\n",
      "\n",
      "Epoch 1100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.4344085366065003e-06\n",
      "test MSELoss: 3.539676223833036e-06\n",
      "\n",
      "Epoch 1200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.4279155206816313e-06\n",
      "test MSELoss: 3.5334763803499906e-06\n",
      "\n",
      "Epoch 1300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.4216449385831773e-06\n",
      "test MSELoss: 3.5275056992153015e-06\n",
      "\n",
      "Epoch 1400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.415589305691397e-06\n",
      "test MSELoss: 3.5217464604405296e-06\n",
      "\n",
      "Epoch 1500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.409726042269375e-06\n",
      "test MSELoss: 3.5161751839041244e-06\n",
      "\n",
      "Epoch 1600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.4040345196844766e-06\n",
      "test MSELoss: 3.5107719365138718e-06\n",
      "\n",
      "Epoch 1700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3984979831618004e-06\n",
      "test MSELoss: 3.505520726321265e-06\n",
      "\n",
      "Epoch 1800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3931022071987006e-06\n",
      "test MSELoss: 3.500406864986871e-06\n",
      "\n",
      "Epoch 1900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.387835849950193e-06\n",
      "test MSELoss: 3.495420408701951e-06\n",
      "\n",
      "Epoch 2000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.382689712138197e-06\n",
      "test MSELoss: 3.490551944196341e-06\n",
      "\n",
      "Epoch 2100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.377655484133751e-06\n",
      "test MSELoss: 3.485793195068254e-06\n",
      "\n",
      "Epoch 2200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3727258334665817e-06\n",
      "test MSELoss: 3.4811365973534216e-06\n",
      "\n",
      "Epoch 2300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3678948580604106e-06\n",
      "test MSELoss: 3.4765769669320433e-06\n",
      "\n",
      "Epoch 2400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3631560795610398e-06\n",
      "test MSELoss: 3.4721073158531605e-06\n",
      "\n",
      "Epoch 2500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.358504959378222e-06\n",
      "test MSELoss: 3.467723126959754e-06\n",
      "\n",
      "Epoch 2600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.353936763300965e-06\n",
      "test MSELoss: 3.463420671323547e-06\n",
      "\n",
      "Epoch 2700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.349447419015711e-06\n",
      "test MSELoss: 3.459193931121263e-06\n",
      "\n",
      "Epoch 2800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.345032689963079e-06\n",
      "test MSELoss: 3.4550400566028355e-06\n",
      "\n",
      "Epoch 2900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3406890647979793e-06\n",
      "test MSELoss: 3.4509561980181996e-06\n",
      "\n",
      "Epoch 3000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3364136014600368e-06\n",
      "test MSELoss: 3.446938156533482e-06\n",
      "\n",
      "Epoch 3100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.332203056047014e-06\n",
      "test MSELoss: 3.4429827034424912e-06\n",
      "\n",
      "Epoch 3200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3280544572026745e-06\n",
      "test MSELoss: 3.4390881864965197e-06\n",
      "\n",
      "Epoch 3300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.323965951160594e-06\n",
      "test MSELoss: 3.435251892369706e-06\n",
      "\n",
      "Epoch 3400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3199351999221322e-06\n",
      "test MSELoss: 3.431472017230893e-06\n",
      "\n",
      "Epoch 3500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.315959903667768e-06\n",
      "test MSELoss: 3.4277454536398485e-06\n",
      "\n",
      "Epoch 3600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.312036743271056e-06\n",
      "test MSELoss: 3.4240698520685934e-06\n",
      "\n",
      "Epoch 3700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.308163980636967e-06\n",
      "test MSELoss: 3.4204434390024594e-06\n",
      "\n",
      "Epoch 3800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3043395450207066e-06\n",
      "test MSELoss: 3.4168633950078704e-06\n",
      "\n",
      "Epoch 3900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.3005622606187543e-06\n",
      "test MSELoss: 3.4133288257483704e-06\n",
      "\n",
      "Epoch 4000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2968296335028917e-06\n",
      "test MSELoss: 3.409837199797039e-06\n",
      "\n",
      "Epoch 4100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.293140196423041e-06\n",
      "test MSELoss: 3.4063871225953335e-06\n",
      "\n",
      "Epoch 4200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2894922538104227e-06\n",
      "test MSELoss: 3.402977002527526e-06\n",
      "\n",
      "Epoch 4300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.285884637232733e-06\n",
      "test MSELoss: 3.3996053086108685e-06\n",
      "\n",
      "Epoch 4400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2823157460019775e-06\n",
      "test MSELoss: 3.396270540179103e-06\n",
      "\n",
      "Epoch 4500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2787840801701186e-06\n",
      "test MSELoss: 3.3929719696364676e-06\n",
      "\n",
      "Epoch 4600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.275289016585862e-06\n",
      "test MSELoss: 3.3897076264111094e-06\n",
      "\n",
      "Epoch 4700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2718287164142213e-06\n",
      "test MSELoss: 3.3864764949006105e-06\n",
      "\n",
      "Epoch 4800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2684026268139655e-06\n",
      "test MSELoss: 3.38327766561027e-06\n",
      "\n",
      "Epoch 4900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2650093417728163e-06\n",
      "test MSELoss: 3.3801111385400873e-06\n",
      "\n",
      "Epoch 5000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.261648044030779e-06\n",
      "test MSELoss: 3.376974943118209e-06\n",
      "\n",
      "Epoch 5100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2583175405848287e-06\n",
      "test MSELoss: 3.3738688216544688e-06\n",
      "\n",
      "Epoch 5200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.25501658324232e-06\n",
      "test MSELoss: 3.370789924398802e-06\n",
      "\n",
      "Epoch 5300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2517444925283035e-06\n",
      "test MSELoss: 3.3677389486304794e-06\n",
      "\n",
      "Epoch 5400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2484998202822637e-06\n",
      "test MSELoss: 3.3647144088414886e-06\n",
      "\n",
      "Epoch 5500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2452826753713964e-06\n",
      "test MSELoss: 3.361716168607624e-06\n",
      "\n",
      "Epoch 5600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2420915735351254e-06\n",
      "test MSELoss: 3.358742317990012e-06\n",
      "\n",
      "Epoch 5700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.238925596773499e-06\n",
      "test MSELoss: 3.355793326894248e-06\n",
      "\n",
      "Epoch 5800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.235784499379304e-06\n",
      "test MSELoss: 3.352867967502486e-06\n",
      "\n",
      "Epoch 5900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2326669110623098e-06\n",
      "test MSELoss: 3.3499647694649564e-06\n",
      "\n",
      "Epoch 6000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2295724810282176e-06\n",
      "test MSELoss: 3.3470843997444415e-06\n",
      "\n",
      "Epoch 6100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2265005356612505e-06\n",
      "test MSELoss: 3.3442255395736235e-06\n",
      "\n",
      "Epoch 6200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.223450661228262e-06\n",
      "test MSELoss: 3.341387779679887e-06\n",
      "\n",
      "Epoch 6300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2204217800196117e-06\n",
      "test MSELoss: 3.338570619841145e-06\n",
      "\n",
      "Epoch 6400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.217413248660411e-06\n",
      "test MSELoss: 3.3357725139164054e-06\n",
      "\n",
      "Epoch 6500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2144246195855193e-06\n",
      "test MSELoss: 3.332993946969509e-06\n",
      "\n",
      "Epoch 6600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2114554590272024e-06\n",
      "test MSELoss: 3.330234145929959e-06\n",
      "\n",
      "Epoch 6700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2085049410312105e-06\n",
      "test MSELoss: 3.327491928454644e-06\n",
      "\n",
      "Epoch 6800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2055722863276657e-06\n",
      "test MSELoss: 3.324766400207106e-06\n",
      "\n",
      "Epoch 6900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.2026574289536308e-06\n",
      "test MSELoss: 3.322058485840292e-06\n",
      "\n",
      "Epoch 7000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1997596268733207e-06\n",
      "test MSELoss: 3.3193660783581437e-06\n",
      "\n",
      "Epoch 7100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1968781648897385e-06\n",
      "test MSELoss: 3.3166899205146668e-06\n",
      "\n",
      "Epoch 7200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.194013194396328e-06\n",
      "test MSELoss: 3.314029148289895e-06\n",
      "\n",
      "Epoch 7300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1911638824456356e-06\n",
      "test MSELoss: 3.3113832159870072e-06\n",
      "\n",
      "Epoch 7400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1883299292748597e-06\n",
      "test MSELoss: 3.3087523055049435e-06\n",
      "\n",
      "Epoch 7500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1855110428704283e-06\n",
      "test MSELoss: 3.306135325450062e-06\n",
      "\n",
      "Epoch 7600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1827068091211825e-06\n",
      "test MSELoss: 3.303532381930078e-06\n",
      "\n",
      "Epoch 7700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.179916901047523e-06\n",
      "test MSELoss: 3.3009424138678395e-06\n",
      "\n",
      "Epoch 7800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1771408170287227e-06\n",
      "test MSELoss: 3.298366118542617e-06\n",
      "\n",
      "Epoch 7900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1743778809919326e-06\n",
      "test MSELoss: 3.2958024803519947e-06\n",
      "\n",
      "Epoch 8000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1716279574201744e-06\n",
      "test MSELoss: 3.293250847491436e-06\n",
      "\n",
      "Epoch 8100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1688907703653465e-06\n",
      "test MSELoss: 3.290711083536735e-06\n",
      "\n",
      "Epoch 8200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1661661448083107e-06\n",
      "test MSELoss: 3.2881828853229916e-06\n",
      "\n",
      "Epoch 8300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1634536150393937e-06\n",
      "test MSELoss: 3.2856662073754707e-06\n",
      "\n",
      "Epoch 8400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.160752720830084e-06\n",
      "test MSELoss: 3.283160943586457e-06\n",
      "\n",
      "Epoch 8500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1580632138270906e-06\n",
      "test MSELoss: 3.280665805505123e-06\n",
      "\n",
      "Epoch 8600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1553848821551924e-06\n",
      "test MSELoss: 3.278181505568985e-06\n",
      "\n",
      "Epoch 8700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.152717487856404e-06\n",
      "test MSELoss: 3.2757076041889378e-06\n",
      "\n",
      "Epoch 8800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.150060703006097e-06\n",
      "test MSELoss: 3.2732435556681595e-06\n",
      "\n",
      "Epoch 8900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1474143820699992e-06\n",
      "test MSELoss: 3.2707894964308557e-06\n",
      "\n",
      "Epoch 9000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1447785159758443e-06\n",
      "test MSELoss: 3.2683446230900395e-06\n",
      "\n",
      "Epoch 9100/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1421522232017516e-06\n",
      "test MSELoss: 3.265909208494122e-06\n",
      "\n",
      "Epoch 9200/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.139535743595763e-06\n",
      "test MSELoss: 3.2634828433704874e-06\n",
      "\n",
      "Epoch 9300/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1369288724648683e-06\n",
      "test MSELoss: 3.2610651639212546e-06\n",
      "\n",
      "Epoch 9400/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.134330846415656e-06\n",
      "test MSELoss: 3.258655942772748e-06\n",
      "\n",
      "Epoch 9500/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1317414006513512e-06\n",
      "test MSELoss: 3.2562545887534117e-06\n",
      "\n",
      "Epoch 9600/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1291610522911364e-06\n",
      "test MSELoss: 3.253861981041458e-06\n",
      "\n",
      "Epoch 9700/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.126589229593226e-06\n",
      "test MSELoss: 3.2514777406807602e-06\n",
      "\n",
      "Epoch 9800/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1240255297867943e-06\n",
      "test MSELoss: 3.249100927860127e-06\n",
      "\n",
      "Epoch 9900/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.1214699853807967e-06\n",
      "test MSELoss: 3.2467313000476378e-06\n",
      "\n",
      "Epoch 10000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.118922595808216e-06\n",
      "test MSELoss: 3.2443690543004777e-06\n",
      "\n",
      "Epoch 10000/10000, Learning rate 1.0000000000000002e-06\n",
      "----------\n",
      "train MSELoss: 3.118922595808216e-06\n",
      "test MSELoss: 3.2443690543004777e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# sparsity and shape of mask\n",
    "# mask_2d=create_mask_2d(m,b,db)\n",
    "connection=torch.tensor(create_mask_2d(m,b,db))\n",
    "\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "\n",
    "    # Set file names\n",
    "    file_name_AE=\"../__model/ex9_wave/ex23_AE_\"+str(f)+\"_swish_seed_\"+str(seed)+\".p\"\n",
    "    file_name_AE_temp=\"../__temp/ex9_wave/ex23_AE_\"+str(f)+\"_swish_temp_seed_\"+str(seed)+\".pkl\"\n",
    "    PATH = '../__temp/ex9_wave/ex23_AE_'+str(f)+\"_swish_checkpoint_seed_\"+str(seed)+\".tar\"\n",
    "    loss_hist_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".p\"\n",
    "    loss_fig_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".png\"\n",
    "    rel_recon_fig_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_rel_recon_seed_\"+str(seed)+\".png\"\n",
    "    avg_rel_err_fig_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_avg_rel_err_seed_\"+str(seed)+\".png\"\n",
    "    max_rel_err_fig_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_max_rel_err_seed_\"+str(seed)+\".png\"\n",
    "    rel_err_fig_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_rel_err_seed_\"+str(seed)+\".png\"\n",
    "\n",
    "    print(file_name_AE)\n",
    "    print(file_name_AE_temp)\n",
    "    print(PATH)\n",
    "    print(loss_fig_name)\n",
    "    print(rel_recon_fig_name)\n",
    "    print(avg_rel_err_fig_name)\n",
    "    print(max_rel_err_fig_name)\n",
    "    print(rel_err_fig_name)\n",
    "    \n",
    "    if activation=='sigmoid':\n",
    "        print('Activation fundtion: sigmoid')\n",
    "        class Encoder(nn.Module):\n",
    "            def __init__(self,m,M1,f):\n",
    "                super(Encoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(m,M1),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.Linear(M1,f,bias=False)\n",
    "                )\n",
    "\n",
    "            def forward(self, y):     \n",
    "                y = y.view(-1,m)\n",
    "                T = self.full(y)\n",
    "                T = T.squeeze()\n",
    "\n",
    "                return T\n",
    "\n",
    "        class Decoder(nn.Module):\n",
    "            def __init__(self,f,M2,m):\n",
    "                super(Decoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(f,M2),\n",
    "                    nn.Sigmoid(),\n",
    "                    # nn.Linear(M2,m,bias=False)\n",
    "                    sl.SparseLinear(M2,m,bias=False,connectivity=connection)\n",
    "                )\n",
    "\n",
    "            def forward(self,T):\n",
    "                T = T.view(-1,f)\n",
    "                y = self.full(T)\n",
    "                y = y.squeeze()\n",
    "\n",
    "                return y\n",
    "\n",
    "    elif activation=='swish':\n",
    "        print('Activation fundtion: swish')\n",
    "        def silu(input):\n",
    "            return input * torch.sigmoid(input)\n",
    "\n",
    "        class SiLU(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "            def forward(self, input):\n",
    "                return silu(input)\n",
    "\n",
    "        class Encoder(nn.Module):\n",
    "            def __init__(self,m,M1,f):\n",
    "                super(Encoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(m,M1),\n",
    "                    SiLU(),\n",
    "                    nn.Linear(M1,f,bias=False)\n",
    "                )\n",
    "\n",
    "            def forward(self, y):     \n",
    "                y = y.view(-1,m)\n",
    "                T = self.full(y)\n",
    "                T = T.squeeze()\n",
    "\n",
    "                return T\n",
    "\n",
    "        class Decoder(nn.Module):\n",
    "            def __init__(self,f,M2,m):\n",
    "                super(Decoder,self).__init__()\n",
    "                self.full = nn.Sequential(\n",
    "                    nn.Linear(f,M2),\n",
    "                    SiLU(),\n",
    "                    # nn.Linear(M2,m,bias=False)\n",
    "                    sl.SparseLinear(M2,m,bias=False,connectivity=connection)\n",
    "                )\n",
    "\n",
    "            def forward(self,T):\n",
    "                T = T.view(-1,f)\n",
    "                y = self.full(T)\n",
    "                y = y.squeeze()\n",
    "\n",
    "                return y\n",
    "    else:\n",
    "        raise NameError('{} is given for option, but it must be either sigmoid or swish'.format(activation))\n",
    "        \n",
    "        \n",
    "    # number of parameters and memory\n",
    "    en_para=m*M1+M1+M1*f\n",
    "    # de_para=f*M2+M2+np.count_nonzero(mask_2d)\n",
    "    de_para=f*M2+M2+connection.shape[1]\n",
    "    # print('Encoder parameters:{:.8e}({:.4}GB)'.format(en_para,en_para*4/2**30),\\\n",
    "    #       'Decoder parameters:{:.8e}({:.4}GB)'.format(de_para,(f*M2+M2+M2*m)*4/2**30))\n",
    "    print('Encoder parameters:{:.8e}({:.4}GB)'.format(en_para,en_para*4/2**30),\\\n",
    "          'Decoder parameters:{:.8e}({:.4}GB)'.format(de_para,de_para*4/2**30))\n",
    "    \n",
    "    # load model\n",
    "    try:\n",
    "        checkpoint = torch.load(PATH, map_location=device)\n",
    "\n",
    "        encoder = Encoder(m,M1,f).to(device)\n",
    "        decoder = Decoder(f,M2,m).to(device)\n",
    "\n",
    "        # # Prune\n",
    "        # prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask_2d).to(device))    \n",
    "\n",
    "        optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=l_rate)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=num_patience) \n",
    "\n",
    "        loss_func = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        last_epoch = checkpoint['epoch']\n",
    "        loss_hist = checkpoint['loss_hist']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        early_stop_counter = checkpoint['early_stop_counter']\n",
    "        best_encoder_wts = checkpoint['best_encoder_wts']\n",
    "        best_decoder_wts = checkpoint['best_decoder_wts']\n",
    "\n",
    "        print(\"\\n--------checkpoint restored--------\\n\")\n",
    "\n",
    "        # resume training\n",
    "        print(\"\")\n",
    "        print('Re-start {}th training... m={}, f={},a={}, b={}, db={}'.format(last_epoch+1, m, f, a, b, db))\n",
    "    except:\n",
    "        encoder = Encoder(m,M1,f).to(device)\n",
    "        decoder = Decoder(f,M2,m).to(device)\n",
    "\n",
    "        # # Prune\n",
    "        # prune.custom_from_mask(decoder.full[2], name='weight', mask=torch.tensor(mask_2d).to(device))\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=l_rate)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,patience=num_patience) \n",
    "\n",
    "        loss_func = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        last_epoch = 0\n",
    "        loss_hist = {'train':[],'test':[]}\n",
    "        best_loss = float(\"inf\")\n",
    "        early_stop_counter = 1\n",
    "        best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "        best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "\n",
    "        print(\"\\n--------checkpoint not restored--------\\n\")\n",
    "\n",
    "        # start training\n",
    "        print(\"\")\n",
    "        print('Start first training... m={}, f={}, a={}, b={}, db={}'.format(m, f, a, b, db))\n",
    "    pass\n",
    "\n",
    "    # train model\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(last_epoch+1,num_epochs+1):   \n",
    "\n",
    "        if epoch%num_epochs_print == 0:\n",
    "            print()\n",
    "            print('Epoch {}/{}, Learning rate {}'.format(\n",
    "                epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and test phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                encoder.train()  # Set model to training mode\n",
    "                decoder.train()  # Set model to training mode\n",
    "            else:\n",
    "                encoder.eval()   # Set model to evaluation mode\n",
    "                decoder.eval()   # Set model to evaluation mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data\n",
    "            for data, in data_loaders[phase]:\n",
    "                inputs = data.to(device)\n",
    "                targets = data.to(device)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = decoder(encoder(inputs))\n",
    "                    loss = loss_func(outputs, targets)\n",
    "\n",
    "                    # backward\n",
    "                    loss.backward()\n",
    "\n",
    "                    # optimize\n",
    "                    optimizer.step()  \n",
    "\n",
    "                    # add running loss\n",
    "                    running_loss += loss.item()*inputs.shape[0]\n",
    "                else:\n",
    "                    with torch.set_grad_enabled(False):\n",
    "                        outputs = decoder(encoder(inputs))\n",
    "                        running_loss += loss_func(outputs,targets).item()*inputs.shape[0]\n",
    "\n",
    "            # compute epoch loss\n",
    "            epoch_loss = running_loss / dataset_shapes[phase][0]\n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "\n",
    "            # update learning rate\n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            if epoch%num_epochs_print == 0:\n",
    "                print('{} MSELoss: {}'.format(\n",
    "                    phase, epoch_loss))\n",
    "\n",
    "        # deep copy the model\n",
    "        if round(loss_hist['test'][-1],10) < round(best_loss,10):\n",
    "            best_loss = loss_hist['test'][-1]\n",
    "            early_stop_counter = 1\n",
    "            best_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "            best_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= early_stop_patience:  \n",
    "                break\n",
    "\n",
    "        # save checkpoint every num_epoch_print\n",
    "        if epoch%num_epochs_print== 0:\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'encoder_state_dict': encoder.state_dict(),\n",
    "                        'decoder_state_dict': decoder.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss_hist': loss_hist,\n",
    "                        'best_loss': best_loss,\n",
    "                        'early_stop_counter': early_stop_counter,\n",
    "                        'best_encoder_wts': best_encoder_wts,\n",
    "                        'best_decoder_wts': best_decoder_wts,\n",
    "                        }, PATH)        \n",
    "\n",
    "    print()\n",
    "    print('Epoch {}/{}, Learning rate {}'.format(epoch, num_epochs, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    print('-' * 10)\n",
    "    print('train MSELoss: {}'.format(loss_hist['train'][-1]))\n",
    "    print('test MSELoss: {}'.format(loss_hist['test'][-1]))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    # load best model weights\n",
    "    encoder.load_state_dict(best_encoder_wts)\n",
    "    decoder.load_state_dict(best_decoder_wts)\n",
    "\n",
    "    # compute best train MSELoss\n",
    "    encoder.to('cpu').eval()\n",
    "    decoder.to('cpu').eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_inputs = torch.tensor(data_u[train_ind])\n",
    "        train_targets = torch.tensor(data_u[train_ind])\n",
    "        train_outputs = decoder(encoder(train_inputs))\n",
    "        train_loss = loss_func(train_outputs,train_targets).item()\n",
    "\n",
    "    # print out training time and best results\n",
    "    print()\n",
    "    if epoch < num_epochs:\n",
    "        print('Early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'\\\n",
    "              .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    else:\n",
    "        print('No early stopping: {}th training complete in {:.0f}h {:.0f}m {:.0f}s'\\\n",
    "              .format(epoch-last_epoch, time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print('-' * 10)\n",
    "    print('Best train MSELoss: {}'.format(train_loss))\n",
    "    print('Best test MSELoss: {}'.format(best_loss))\n",
    "\n",
    "    # save models\n",
    "    print()\n",
    "    print(\"Saving after {}th training to\".format(epoch),file_name_AE_temp)\n",
    "    torch.save((encoder,decoder),file_name_AE_temp)\n",
    "    \n",
    "    # delete checkpoint\n",
    "    try:\n",
    "        os.remove(PATH)\n",
    "        print()\n",
    "        print(\"checkpoint removed\")\n",
    "    except:\n",
    "        print(\"no checkpoint exists\") \n",
    "\n",
    "    # release gpu memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # plot train and test loss\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.semilogy(loss_hist['train'])\n",
    "    plt.semilogy(loss_hist['test'])\n",
    "    plt.legend(['train','test'])\n",
    "    # plt.show()   \n",
    "    plt.savefig(loss_fig_name)\n",
    "\n",
    "    # load models\n",
    "    try:\n",
    "        encoder_u,decoder_u = torch.load(file_name_AE_temp,map_location='cpu')\n",
    "        print(\"\\n--------model restored--------\\n\")\n",
    "    except:\n",
    "        print(\"\\n--------model not restored--------\\n\")\n",
    "        pass\n",
    "\n",
    "    # load weights and bias\n",
    "    en_wu1_s=encoder_u.full[0].weight.detach().numpy().astype('float32')\n",
    "    en_bu1=encoder_u.full[0].bias.detach().numpy().astype('float32')\n",
    "    en_wu2=encoder_u.full[2].weight.detach().numpy().astype('float32')\n",
    "    de_wu1=decoder_u.full[0].weight.detach().numpy().astype('float32')\n",
    "    de_bu1=decoder_u.full[0].bias.detach().numpy().astype('float32')\n",
    "    # de_wu2_s=decoder_u.full[2].weight.detach().numpy().astype('float32')\n",
    "    # de_wu2_s_sp=sp.csr_matrix(de_wu2_s,dtype='float32')\n",
    "    aaa=decoder.full[2].weight.detach()\n",
    "    de_wu2_s_sp=sp.csr_matrix((aaa.values(),(aaa.indices()[0],aaa.indices()[1])),\n",
    "                            shape=aaa.size(),dtype='float32')\n",
    "    de_wu2_s=de_wu2_s_sp.toarray().astype('float32')\n",
    "\n",
    "    # rescale weights\n",
    "    en_wu1=en_wu1_s*u_scale_reciprocal\n",
    "    de_wu1T=de_wu1.T\n",
    "    de_wu2T=u_scale*de_wu2_s.T\n",
    "    de_wu2=de_wu2T.T\n",
    "    de_wu2_sp=sp.csr_matrix(de_wu2,dtype='float32')\n",
    "    de_wu2T_sp=de_wu2_sp.T\n",
    "    \n",
    "    # save weights and references   \n",
    "    AE={'en_wu1':en_wu1,'en_bu1':en_bu1,'en_wu2':en_wu2,\n",
    "        'de_wu1':de_wu1,'de_bu1':de_bu1,'de_wu2':de_wu2,\n",
    "        'de_wu1T':de_wu1T,'de_wu2T':de_wu2T,'de_wu2_sp':de_wu2_sp,'de_wu2T_sp':de_wu2T_sp,'u_ref':u_ref,}\n",
    "\n",
    "    with open(file_name_AE,'wb') as ffff:\n",
    "        pickle.dump(AE,ffff)\n",
    "        \n",
    "    # save loss history\n",
    "    with open(loss_hist_name,'wb') as f5:\n",
    "        pickle.dump(loss_hist,f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete checkpoint\n",
    "try:\n",
    "    os.remove(PATH)\n",
    "    print()\n",
    "    print(\"checkpoint removed\")\n",
    "except:\n",
    "    print(\"no checkpoint exists\") \n",
    "\n",
    "# release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# plot train and test loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.semilogy(loss_hist['train'],linestyle='dotted',linewidth=2)\n",
    "plt.semilogy(loss_hist['test'],linestyle='dashdot',linewidth=2)\n",
    "plt.legend(['train','test'],fontsize=24)\n",
    "plt.xlabel(\"Epochs\",fontsize=28)\n",
    "plt.ylabel(\"MSE\",fontsize=28)\n",
    "# plt.show()   \n",
    "plt.savefig(loss_fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "    \n",
    "    loss_hist_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".p\"\n",
    "    loss_fig_name=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_loss_seed_\"+str(seed)+\".png\"\n",
    "    \n",
    "    with open(loss_hist_name,'rb') as f6:\n",
    "        loss_hist_name=pickle.load(f6)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.semilogy(loss_hist['train'],linestyle='dotted',linewidth=2)\n",
    "    plt.semilogy(loss_hist['test'],linestyle='dashdot',linewidth=2)\n",
    "    plt.legend(['train','test'],fontsize=24)\n",
    "    plt.xlabel(\"Epochs\",fontsize=28)\n",
    "    plt.ylabel(\"MSE\",fontsize=28)\n",
    "    # plt.show()   \n",
    "    plt.savefig(loss_fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params = np.arange(80,121,5)\n",
    "\n",
    "# load snapshot\n",
    "snapshot = np.array([])\n",
    "for i in training_params:\n",
    "    ex = np.load('../__data/ex9_wave/ex23_interp_{}.npz'.format(i), allow_pickle = True)\n",
    "    ex = ex.f.arr_0\n",
    "    snapshot = np.append(snapshot, ex)\n",
    "snapshot = snapshot.reshape(len(training_params)*(nt+1),-1).astype('float32')\n",
    "\n",
    "# number of data points\n",
    "ndata = snapshot.shape[0]\n",
    "\n",
    "orig_data_u=np.copy(snapshot)\n",
    "\n",
    "# f_list=np.array([3,4,5,6])\n",
    "print(\"Latent Sapce Dim.: {}\".format(f_list))\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "\n",
    "    # Set file names\n",
    "    file_name_AE=\"../__model/ex9_wave/ex23_AE_\"+str(f)+\"_swish_seed_\"+str(seed)+\".p\"\n",
    "    print(file_name_AE)\n",
    "\n",
    "    with open(file_name_AE,'rb') as fff:\n",
    "        AE = pickle.load(fff)\n",
    "\n",
    "    en_wu1=AE['en_wu1']\n",
    "    en_bu1=AE['en_bu1']\n",
    "    en_wu2=AE['en_wu2']\n",
    "    de_wu1=AE['de_wu1']\n",
    "    de_bu1=AE['de_bu1']\n",
    "    de_wu2=AE['de_wu2']\n",
    "    de_wu1T=AE['de_wu1T']\n",
    "    de_wu2T=AE['de_wu2T']\n",
    "    de_wu2_sp=AE['de_wu2_sp']\n",
    "    de_wu2T_sp=AE['de_wu2T_sp']\n",
    "    u_ref=AE['u_ref']\n",
    "\n",
    "    latent_dim=de_wu1.shape[1]\n",
    "\n",
    "    # numpy version of AE\n",
    "    def sigmoid_np(input):\n",
    "        return (1.0/(1.0+np.exp(-input))).astype('float32')\n",
    "\n",
    "    def encoder_u_np_forward(x):\n",
    "        z1 = en_wu1.dot(x) + en_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = en_wu2.dot(a1)   \n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = de_wu2.dot(a1)  \n",
    "        return y\n",
    "\n",
    "    def decoder_u_sp_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = de_wu2.dot(a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = dout.dot(de_wu2T)   \n",
    "        return y,dydxT\n",
    "\n",
    "    def decoder_u_sp_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = sp.csr_matrix.dot(dout,de_wu2T_sp)\n",
    "        return y,dydxT\n",
    "\n",
    "    # compute MSELoss\n",
    "    comp_orig_data_u=np.zeros((ndata,f))\n",
    "    rest_orig_data_u=np.zeros(orig_data_u.shape)\n",
    "\n",
    "    for k in range(ndata):\n",
    "        comp_orig_data_u[k]=encoder_u_np_forward(orig_data_u[k]-u_ref)\n",
    "        rest_orig_data_u[k]=decoder_u_sp_forward(comp_orig_data_u[k]) + u_ref\n",
    "\n",
    "    print(\"MSELoss of AE (rescaled): {:.8e}\".format(\n",
    "        np.linalg.norm(orig_data_u-rest_orig_data_u)**2/np.prod(orig_data_u.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection Error and Residual Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list=np.array([3,4,5,6]); \n",
    "# seed=0\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i] # latent dimension\n",
    "\n",
    "    # Set file names\n",
    "    file_name_AE=\"../__model/ex9_wave/ex23_AE_\"+str(f)+\"_swish_seed_\"+str(seed)+\".p\"\n",
    "    file_path_prj_result = \"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_proj_result_seed_\"+str(seed)+\".p\"\n",
    "    file_path_residual=\"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_residual_seed_\"+str(seed)+\".p\"\n",
    "    file_path_residual_SVD=\"../__model/ex9_wave/ex23_AE_\"+str(f)+\"_swish_residual_SVD_seed_\"+str(seed)+\".p\"\n",
    "\n",
    "    print(file_name_AE)\n",
    "    print(file_path_prj_result)\n",
    "    print(file_path_residual)\n",
    "    print(file_path_residual_SVD)\n",
    "    \n",
    "    with open(file_name_AE,'rb') as fff:\n",
    "        AE = pickle.load(fff)\n",
    "\n",
    "    en_wu1=AE['en_wu1']\n",
    "    en_bu1=AE['en_bu1']\n",
    "    en_wu2=AE['en_wu2']\n",
    "    de_wu1=AE['de_wu1']\n",
    "    de_bu1=AE['de_bu1']\n",
    "    de_wu2=AE['de_wu2']\n",
    "    de_wu1T=AE['de_wu1T']\n",
    "    de_wu2T=AE['de_wu2T']\n",
    "    de_wu2_sp=AE['de_wu2_sp']\n",
    "    de_wu2T_sp=AE['de_wu2T_sp']\n",
    "    u_ref=AE['u_ref']\n",
    "\n",
    "    latent_dim=de_wu1.shape[1]\n",
    "\n",
    "    # numpy version of AE\n",
    "    def sigmoid_np(input):\n",
    "        return (1.0/(1.0+np.exp(-input))).astype('float32')\n",
    "\n",
    "    def encoder_u_np_forward(x):\n",
    "        z1 = en_wu1.dot(x) + en_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = en_wu2.dot(a1)   \n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = de_wu2.dot(a1)  \n",
    "        return y\n",
    "\n",
    "    def decoder_u_sp_forward(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        a1 = z1 * sigmoid_np(z1)\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "        return y\n",
    "\n",
    "    def decoder_u_np_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = de_wu2.dot(a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = dout.dot(de_wu2T)   \n",
    "        return y,dydxT\n",
    "\n",
    "    def decoder_u_sp_forward_backwardT(x):\n",
    "        z1 = de_wu1.dot(x) + de_bu1\n",
    "        sigma = sigmoid_np(z1)\n",
    "        a1 = z1 * sigma\n",
    "        y = sp.csr_matrix.dot(de_wu2_sp,a1)\n",
    "\n",
    "        dout = de_wu1T\n",
    "        dout = (sigma+a1*(1-sigma))*dout\n",
    "        dydxT = sp.csr_matrix.dot(dout,de_wu2T_sp)\n",
    "        return y,dydxT\n",
    "    \n",
    "    results={'avg_rel_err':[],'rel_err':[],'elapsed_time':[]}\n",
    "    residual_snapshot=[]\n",
    "\n",
    "    for FOM_parameter in training_params:\n",
    "        # Load FOM solution\n",
    "        ex = np.load('../__data/ex9_wave/ex23_interp_{}.npz'.format(FOM_parameter), allow_pickle = True)\n",
    "        ex = ex.f.arr_0\n",
    "        u_full = ex.reshape(nt+1,-1).astype('float32')\n",
    "\n",
    "        # take measurments\n",
    "        um = u_full[:,msmt_idx]\n",
    "\n",
    "        # Initial condition\n",
    "        u0=u_full[0]\n",
    "        u_hat0=encoder_u_np_forward(u0.astype('float32')-u_ref)\n",
    "\n",
    "        # gappyAE\n",
    "        t_start_gappyAE=time.time()\n",
    "\n",
    "        # solution\n",
    "        u_reduced=np.zeros((nt+1,latent_dim))\n",
    "        u_gappyAE=np.zeros((nt+1,ny*nx))\n",
    "\n",
    "        # IC\n",
    "        u_reduced[0]=np.copy(u_hat0)\n",
    "        u_gappyAE[0]=np.copy(u0)\n",
    "\n",
    "        residual=u_full[0]-(u_ref+decoder_u_sp_forward(u_hat0))\n",
    "        residual_snapshot.append(residual)\n",
    "        for k in range(nt):\n",
    "#             print(\"\")\n",
    "#             print(k,\"th time step:\")\n",
    "\n",
    "            u_hatw=np.copy(u_reduced[k])\n",
    "\n",
    "            umw,Jg_umT=decoder_u_sp_forward_backwardT(u_hatw)\n",
    "\n",
    "            umw += u_ref[msmt_idx]\n",
    "\n",
    "            Jg_um_pinv=np.linalg.pinv(Jg_umT.T)\n",
    "\n",
    "            r_um_hat = um[k+1]-umw\n",
    "\n",
    "            res = np.linalg.norm(Jg_umT.dot(r_um_hat))\n",
    "            res_hist=[res]\n",
    "            residual=u_full[k+1]-(u_ref+decoder_u_sp_forward(u_hatw))\n",
    "            residual_snapshot.append(residual)\n",
    "            for itr in range(maxitr):\n",
    "                du_hatw = Jg_um_pinv.dot(r_um_hat)\n",
    "\n",
    "                u_hatw += du_hatw\n",
    "\n",
    "                umw,Jg_umT=decoder_u_sp_forward_backwardT(u_hatw)\n",
    "\n",
    "                umw += u_ref[msmt_idx]\n",
    "\n",
    "                Jg_um_pinv=np.linalg.pinv(Jg_umT.T)\n",
    "\n",
    "                r_um_hat = um[k+1]-umw\n",
    "\n",
    "                res = np.linalg.norm(Jg_umT.dot(r_um_hat))\n",
    "                res_hist.append(res)\n",
    "                residual=u_full[k+1]-(u_ref+decoder_u_sp_forward(u_hatw))\n",
    "                residual_snapshot.append(residual)\n",
    "#                 print(itr,\"th Newton iteration\", \"res:\", \"{:.8e}\".format(np.linalg.norm(residual)))\n",
    "\n",
    "                if res<tol:\n",
    "                    break\n",
    "\n",
    "            u_reduced[k+1]=u_hatw.copy()\n",
    "            u_gappyAE[k+1]=u_ref+decoder_u_sp_forward(u_reduced[k+1])\n",
    "\n",
    "        # elapsed time\n",
    "        t_elapsed_gappyAE=time.time()-t_start_gappyAE\n",
    "        print()\n",
    "        print('Time elapsed: {} sec'.format(t_elapsed_gappyAE))\n",
    "\n",
    "        # error\n",
    "        u_rel_err_gappyAE=np.linalg.norm(u_full-u_gappyAE,ord=2,axis=1)/np.linalg.norm(u_full,ord=2,axis=1)*100\n",
    "        u_avg_rel_err=np.sqrt(np.sum(np.linalg.norm(u_full-u_gappyAE,ord=2,axis=1)**2))/np.sqrt(np.sum(np.linalg.norm(u_full,ord=2,axis=1)**2))*100\n",
    "\n",
    "        print(\"average relative error of u: {}%\".format(u_avg_rel_err))\n",
    "        print(\"maximum relative error of u: {}%\".format(np.max(u_rel_err_gappyAE)))\n",
    "\n",
    "        # save result\n",
    "        results['avg_rel_err'].append(u_avg_rel_err)\n",
    "        results['rel_err'].append(u_rel_err_gappyAE)\n",
    "        results['elapsed_time'].append(t_elapsed_gappyAE)\n",
    "\n",
    "    results['avg_rel_err']=np.array(results['avg_rel_err'])\n",
    "    results['rel_err']=np.array(results['rel_err'])\n",
    "    results['elapsed_time']=np.array(results['elapsed_time'])\n",
    "\n",
    "    with open(file=file_path_prj_result, mode='wb') as ff:\n",
    "        pickle.dump(results, ff)\n",
    "        print(file_path_prj_result)\n",
    "\n",
    "    residual_snapshot=np.array(residual_snapshot)    \n",
    "    with open(file=file_path_residual, mode='wb') as fff:\n",
    "        pickle.dump(residual_snapshot, fff)\n",
    "        print(file_path_residual)\n",
    "\n",
    "    # do svd decomposition\n",
    "    U,S,VT=np.linalg.svd(residual_snapshot.T,full_matrices=False)\n",
    "\n",
    "    SVD={'U':U,'S':S,'VT':VT}\n",
    "\n",
    "    with open(file=file_path_residual_SVD, mode='wb') as fff:\n",
    "        pickle.dump(SVD,fff)  \n",
    "        print(file_path_residual_SVD)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot other cases together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=20)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=24)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=24)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=20)    # legend fontsize\n",
    "plt.rc('figure', titlesize=28)  # fontsize of the figure title\n",
    "\n",
    "linestyle=['solid','dotted','dashed','dashdot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list=np.array([3,4,5,6])\n",
    "\n",
    "plt.figure(num=1,figsize=(10,6))\n",
    "plt.figure(num=2,figsize=(10,6))\n",
    "for i in range(len(f_list)):\n",
    "    f = f_list[i]\n",
    "    file_path_prj_result = \"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_proj_result_seed_\"+str(seed)+\".p\"\n",
    "    with open(file=file_path_prj_result, mode='rb') as ff:\n",
    "        results=pickle.load(ff)   \n",
    "    plt.figure(num=1)\n",
    "    plt.plot(training_params/100,results['avg_rel_err'],linestyle=linestyle[i],linewidth=4)\n",
    "\n",
    "    plt.figure(num=2)\n",
    "    plt.plot(training_params/100,results['rel_err'].max(axis=1),linestyle=linestyle[i],linewidth=4)\n",
    "\n",
    "plt.figure(num=1)    \n",
    "plt.xlabel('FOM parameter')\n",
    "plt.ylabel('Avg. Rel. Err.')\n",
    "plt.legend([\"Latent Space Dim. \"+str(f) for f in f_list])\n",
    "plt.savefig(\"../__result/ex9_wave/ex23_AE_swish_proj_avg_rel_err_seed_\"+str(seed)+\".png\")\n",
    "\n",
    "plt.figure(num=2)\n",
    "plt.xlabel('FOM parameter')\n",
    "plt.ylabel('Max. Rel. Err.')\n",
    "plt.legend([\"Latent Space Dim. \"+str(f) for f in f_list])\n",
    "plt.savefig(\"../__result/ex9_wave/ex23_AE_swish_proj_max_rel_err_seed_\"+str(seed)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_list)\n",
    "print(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=3; j=-1        \n",
    "# f=f_list[i]\n",
    "# param=training_params[j]\n",
    "\n",
    "# file_path_prj_result = \"../__result/ex9_wave/ex23_AE_\"+str(f)+\"_swish_proj_result_seed_\"+str(seed)+\".p\"\n",
    "# with open(file=file_path_prj_result, mode='rb') as ff:\n",
    "#     results=pickle.load(ff)\n",
    "\n",
    "# u_rel_err_gappyAE=results['rel_err'][j,1:]\n",
    "# plt.figure(figsize=(10,5))\n",
    "\n",
    "# plt.plot(u_rel_err_gappyAE.flatten())\n",
    "# plt.xlabel('Data Point')\n",
    "# plt.ylabel('Relative Error (%)')\n",
    "# plt.title('Proj. Err. Latent Space Dim.={}, Param = {}'.format(f,param))\n",
    "# plt.show()\n",
    "\n",
    "# # plot original data\n",
    "# vmin=-1; vmax=1\n",
    "\n",
    "# # AE\n",
    "# plt.figure(figsize=(20,4))\n",
    "\n",
    "# plt.subplot(1,5,1)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[0].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t=0')\n",
    "\n",
    "# plt.subplot(1,5,2)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[int(nt/4)].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*int(nt/4)))\n",
    "\n",
    "# plt.subplot(1,5,3)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[int(nt/4)*2].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*int(nt/4)*2))\n",
    "\n",
    "# plt.subplot(1,5,4)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[int(nt/4)*3].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*int(nt/4)*3))\n",
    "\n",
    "# plt.subplot(1,5,5)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_gappyAE[nt].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('AE @ t={}'.format(dt*nt))\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # FOM\n",
    "# plt.figure(figsize=(20,4))\n",
    "\n",
    "# plt.subplot(1,5,1)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[0].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t=0')\n",
    "\n",
    "# plt.subplot(1,5,2)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[int(nt/4)].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*int(nt/4)))\n",
    "\n",
    "# plt.subplot(1,5,3)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[int(nt/4)*2].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*int(nt/4)*2))\n",
    "\n",
    "# plt.subplot(1,5,4)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[int(nt/4)*3].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*int(nt/4)*3))\n",
    "\n",
    "# plt.subplot(1,5,5)\n",
    "# plt.pcolor(x.reshape(ny,nx), y.reshape(ny,nx), u_full[nt].reshape(ny,nx),vmin=vmin,vmax=vmax)\n",
    "# plt.axis('square')\n",
    "# plt.title('FOM @ t={}'.format(dt*nt))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
